## name: test/sql/copy/s3/upload_config.test_slow
## description: Various working and non-working configs for the uploader
## group: [s3]
#
#require parquet
#
#require httpfs
#
#require-env S3_TEST_SERVER_AVAILABLE 1
#
## override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
#set ignore_error_messages
#
## we want to fail on OOM in this test
#statement ok
#PRAGMA temp_directory=''
#
#statement ok
#SET memory_limit='500MB';
#
#statement ok
#SET s3_secret_access_key='minio_duckdb_user_password';SET s3_access_key_id='minio_duckdb_user';SET s3_region='eu-west-1'; SET s3_endpoint='duckdb-minio.com:9000';SET s3_use_ssl=false;
#
## ~0.5 GB size table, results in ~1GB csv
#statement ok
#CREATE VIEW test_table AS SELECT i::INT32 as a, i::INT32 as b FROM range(62500000) tbl(i);
#
## First try a valid config: 1TB in 10000 parts will have a fairly reasonable 100 MB per part.
## since we only have 2 upload threads, the uploader should need at most 200MB.
#statement ok
#SET s3_uploader_max_parts_per_file=10000;
#
#statement ok
#SET s3_uploader_max_filesize='1TB';
#
#statement ok
#SET s3_uploader_thread_limit = 2;
#
#statement ok
#COPY test_table TO 's3://test-bucket/multipart/export_out_largeparts.csv' WITH (HEADER 1, DELIMITER '|');
#
#query II
#select avg(a), avg(b) from 's3://test-bucket/multipart/export_out_largeparts.csv';
#----
#31249999.5	31249999.5
#
#halt
#
## Now set an invalid config: there should is not enough memory to support this config as a single upload buffer would require
## 1GB of memory, meaning that this will attempt to buffer the whole CSV, which doesn't fit in our 500MB limit.
#statement ok
#SET s3_uploader_max_parts_per_file=1000;
#
#statement ok
#SET s3_uploader_max_filesize='1TB';
#
#statement ok
#SET s3_uploader_thread_limit = 1;
#
#statement error
#COPY lineitem TO 's3://test-bucket/multipart/export_out_should_not_exist.csv' WITH (HEADER 1, DELIMITER '|');
#
## Uploading with limited memory: part size is 100MB, memory limit is 300MB, this should cause allocation to fail requiring the
## uploader to wait for memory to become available. We repeat this test a few times to confirm the memory is freed properly
#foreach iteration 1 2 3 4 5
#
#statement ok
#SET memory_limit='300MB';
#
#statement ok
#SET s3_uploader_max_parts_per_file=10000;
#
#statement ok
#SET s3_uploader_max_filesize='1TB';
#
#statement ok
#SET s3_uploader_thread_limit=10;
#
#statement ok
#COPY lineitem TO 's3://test-bucket/multipart/export_out_limited_memory${iteration}.csv' WITH (HEADER 1, DELIMITER '|');
#
#endloop